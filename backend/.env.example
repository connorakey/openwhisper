# If you are not using Ollama, please leave the following two variables empty
OLLAMA_SERVER_URL=YOUR_OLLAMA_SERVER_URL_HERE
OLLAMA_SERVER_PORT=YOUR_PORT

# If you are not using LM studio, please leave the following two variables empty
LM_STUDIO_SERVER_URL=YOUR_LM_STUDIO_SERVER_URL_HERE
LM_STUDIO_SERVER_PORT=YOUR_PORT

# Change to either 'ollama' or 'lm_studio' depending on which LLM server you want to use
LLM_PROVIDER=YOUR_LLM_PROVIDER_HERE

# Change to the model name you want to use from your LLM server (ensure the model is already installed on the server)
LLM_MODEL_NAME=YOUR_LLM_MODEL_NAME_HERE